{"0": {
    "doc": "1. Introduction to data science and machine learning",
    "title": "Day 1 - Introduction to data science and machine learning",
    "content": "In addition to learning the basics of programming and machine learning, you'll also have to predict the number of visitors to Bristol's museums based on the weather forecast. Note . The first day of the course aims to homogenize levels and teach foundational concepts in data science and machine learning (ML) through hands-on turorials. We recommend all attendants to carefully follow its content. If the content is too easy for you, please tell your scientific instructors and they will provide you with additional resources. Inversely, if the content is too hard for you, please tell your scientific instructors and they will provide you with additional explanations. ",
    "url": "/content/tutorials/1_intro_ml/#day-1---introduction-to-data-science-and-machine-learning",
    
    "relUrl": "/content/tutorials/1_intro_ml/#day-1---introduction-to-data-science-and-machine-learning"
  },"1": {
    "doc": "1. Introduction to data science and machine learning",
    "title": "Table of contents",
    "content": ". | A) Brief : Module introduction | B) Lecture : Introduction to AI and machine learning | C) Hands-on : train your first image classifier with Marcelle . | Data collection | Training | Testing | Deployment | . | D) Tutorials in Python . | How to download and launch the tutorials ? | What will you learn ? | . | E) For advanced students: Create your own interactive ML web application | . ",
    "url": "/content/tutorials/1_intro_ml/#table-of-contents",
    
    "relUrl": "/content/tutorials/1_intro_ml/#table-of-contents"
  },"2": {
    "doc": "1. Introduction to data science and machine learning",
    "title": "A) Brief : Module introduction",
    "content": "Please find the slides of the module description and the schedule of the course below: . Slides : Module introduction . ",
    "url": "/content/tutorials/1_intro_ml/#a-brief--module-introduction",
    
    "relUrl": "/content/tutorials/1_intro_ml/#a-brief--module-introduction"
  },"3": {
    "doc": "1. Introduction to data science and machine learning",
    "title": "B) Lecture : Introduction to AI and machine learning",
    "content": "Please find the slides of the lecture on the introduction to AI and machine learning below: . Slides : Introduction to AI and machine learning . ",
    "url": "/content/tutorials/1_intro_ml/#b-lecture--introduction-to-ai-and-machine-learning",
    
    "relUrl": "/content/tutorials/1_intro_ml/#b-lecture--introduction-to-ai-and-machine-learning"
  },"4": {
    "doc": "1. Introduction to data science and machine learning",
    "title": "C) Hands-on : train your first image classifier with Marcelle",
    "content": "Let’s practice ! . Go to the Marcelle app! . You can find the source files of this activity in the folder train_your_first_classifier of the github repository of the course. Data collection . The first step of the development cycle of ML is data collection. It consists in collecting and annotating data samples that can be used by an ML algorithm to learn a mapping from inputs to outputs. In the previous example, the data samples are images collected with your webcam and the corresponding labels you provided. If pairs of input and output are provided in the training set, we talk about supervised learning. If only inputs are provided, we talk about unsupervised learning. The data samples are usually gathered in two sets: the training set and the test set. The training set is used to train the ML model, while the test set is used to evaluate the performance of the trained model. In other words, the training set is the exemples you work on during the semester, while the test set is the final exam. We provide various miniature image datasets that represent classification problems in engineering, medecine, and public health: . | miniMASK: a dataset of 3 classes of images (with masks, without masks, and with masks incorrectly worn); | miniROAD: a dataset of 3 classes or green, orange, and red traffic lights; | miniTRASH: a dataset of 3 classes of images (packaging, transparent glass, and opaque glass); | miniRETINA: a medical dataset of 3 classes of retinoscopic images (healthy, macular degeneration, and diabetic retinopathy); | miniSKIN: a medical dataset of 2 classes of dermatoscopic images (benign and malignant skin lesions); | . On the Data collection page, click on one button to select the dataset you want to work with for the rest of the tutorial. Hints . Take the time observe each images (by clicking on the thumbnails). Are the differences between each classes obvious to you? . Training . The training phase comprises three steps: . C.1. Features selection : in our case, we will use a pre-trained neural network called MobileNet that extract 1024 features from images. These features were learned from a large dataset of images (ImageNet) and can be used to represent any image. C.2. Model selection : Many machine learning models exist and were developed. In our case, we will use a model called multi-layer perceptron (MLP). It is a simple artificial neural network composed of an input layer (in our case, the 1024 features from MobileNe, some hidden layers that we can choose, and an output layer (the number of classes). Two other parameters are important to set. The batchSize and the number of epochs. An artificial neural network can updates its neurons using several examples at the same time. The batch size indicates the number of images that will be used at each round to update the neurons. The number of epochs indicates the number of times the whole training set will be used to update the neurons. The higher the number of epochs, the more the model will be trained. However, if the number of epochs is too high, the model might overfit the training set and will not be able to generalize to new data. You can choose the number of hidden layers and neurons per layer, the batch size, and the number of epochs on the Training page of the application. C.3. Training : Click on the training button to start the neural network optimization. You will then see two different learning curves appearing. The represent the losses and accuracies as a function of the number of epochs. The loss is a measure of the error between the predictions of the model and the true labels. The accuracy is the percentage of correct predictions. A validation loss and accuracy are computed using a portion of the training set that is not used for the training. If the validation loss and validation accuracy are not improving, it means that the model is overfitting the training set and that it will not generalize well to new data. In this case, you should stop the training and reflect on the parameters of the model or the data used for the task. Testing . At this point, the model only saw the training set, eventhough it artificially splited this set into a training and validation set. To really assess the model performance, we need to compute a test accuracy on unseen images. On the Testing page of the application, you can see two confusion matrices. On the left is shown the global accuracy and a confusion matrix computed on the training set only. The rows of the confusion matrix represent the true labels (what should be predicted), while the columns represent the predicted labels. The diagonal of the matrix represents the number of correct predictions. A confusion matrix gives a finer view of which class is confused with which class. On the right, you can see the global accuracy and a confusion matrix computed on the test set only. The test accuracy is a better indicator of the model performance. If the test accuracy is much lower than the training accuracy, it means that the model is overfitting the training set and that it will not generalize well to new data. Questions . | For the task you selected, what averaged accuracy would you obtain with a random classifer? Is the trained classifier better than a random classifier in your case? | What difference do you observe between the training and test accuracies? What does it mean? | Which classes are the most confused? Why do you think so? Among the confusions you identified, is there any would be more problematic than others in the problem selected? | What would you do to improve the model performance? | . Deployment . Now you trained and test a machine learning model, you can learn more about its behavior by observing its predictions on particular images. On the Deployment page of the application, you can see the predictions of the model when you click on the thumbnails of the training or test set. Machine learning models can be noisy and biased. Noisiness indicates that the model is not stable and that it can give different predictions for similar inputs (it’s unpredictably wrong). Bias indicates that the model is wrong or unfair for similar inputs (it’s always wrong in the same way). Biases in ML models can lead to discrimination as illustrated in many different controversies over the past years. These incidents are documented on a public website called AI incident database. Questions . Among erroneous predictions, can you identify biases? Are these biases explained by the training data? Are these biases problematic in the problem selected? . Now you know the elementary steps to train and test a machine learning model, you can try the same process on another dataset. The next section will teach you how to conduct the same steps using a programming langage (Python) and dedicated ML libraries. ",
    "url": "/content/tutorials/1_intro_ml/#c-hands-on--train-your-first-image-classifier-with-marcelle",
    
    "relUrl": "/content/tutorials/1_intro_ml/#c-hands-on--train-your-first-image-classifier-with-marcelle"
  },"5": {
    "doc": "1. Introduction to data science and machine learning",
    "title": "D) Tutorials in Python",
    "content": "How to download and launch the tutorials ? . First go to the github repository of the course: . Github repository of the course . Then, download the repository on your computer. In a terminal, launch the following command: . git clone https://github.com/aica-wavelab/aica-assignments.git . If you struggle with terminal commands, you can also download the repository as a zip file by clicking on the green button Code on the top right of the github repository page, then Download ZIP. Now open the folder A1_introduction and launch the jupyter notebook server by typing the following command in the terminal: . jupyter notebook . Then click on the first file 1_tutorial_jupyter_notebook.ipynb to open the tutorial and follow the instructions. Alternatively, you can launch the Anaconda navigator (desktop application) and open the jupyter notebook application from there. Then, navigate to the folder A1_introduction and open the first file 1_tutorial_jupyter_notebook.ipynb. What will you learn ? . For the first day, all tutorials are in Python and use jupyter notebooks. This folder A1_introduction contains: . | A tutorial on jupyter notebooks: 1_tutorial_jupyter_notebook.ipynb | A tutorial on the basics of the Python programming language: 2_tutorial_python.ipynb | A tutorial on data science basics using the pandas library and seaborn library: 3_tutorial_data_science.ipynb. You will apply data sciences techniques to analyze the curation of the Museum of Modern Art (MoMA) in New York City, USA. | A tutorial to train your first classifier in Python using the keras library: 4_tutorial_machine_learning.ipynb. You will apply machine learning techniques to predict the number of visitors in the museums of Bristol, UK. | . ",
    "url": "/content/tutorials/1_intro_ml/#d-tutorials-in-python",
    
    "relUrl": "/content/tutorials/1_intro_ml/#d-tutorials-in-python"
  },"6": {
    "doc": "1. Introduction to data science and machine learning",
    "title": "E) For advanced students: Create your own interactive ML web application",
    "content": "The interactive applications you used were programmed using Marcelle. Marcelle is a modular open source toolkit for programming interactive machine learning applications. Marcelle is built around components embedding computation and interaction that can be composed to form reactive machine learning pipelines and custom user interfaces. This architecture enables rapid prototyping and extension. Marcelle can be used to build interfaces to Python scripts, and it provides flexible data stores to facilitate collaboration between machine learning experts, designers and end users. If you want to learn how to create your own interactive machine learning application, please read the introduction and follow the tutorial on the Marcelle website: . Marcelle tutorial . ",
    "url": "/content/tutorials/1_intro_ml/#e-for-advanced-students-create-your-own-interactive-ml-web-application",
    
    "relUrl": "/content/tutorials/1_intro_ml/#e-for-advanced-students-create-your-own-interactive-ml-web-application"
  },"7": {
    "doc": "1. Introduction to data science and machine learning",
    "title": "1. Introduction to data science and machine learning",
    "content": " ",
    "url": "/content/tutorials/1_intro_ml/",
    
    "relUrl": "/content/tutorials/1_intro_ml/"
  },"8": {
    "doc": "2. Image classification of museum artefacts",
    "title": "Day 2 - Image classification of museum artefact",
    "content": "Example of artefacts from the Museum Art Medium (MAMe) dataset you will be working on&lt;/a&gt; . ",
    "url": "/content/tutorials/2_image_classification/#day-2---image-classification-of-museum-artefact",
    
    "relUrl": "/content/tutorials/2_image_classification/#day-2---image-classification-of-museum-artefact"
  },"9": {
    "doc": "2. Image classification of museum artefacts",
    "title": "Table of contents",
    "content": ". | Introduction | Dataset | . ",
    "url": "/content/tutorials/2_image_classification/#table-of-contents",
    
    "relUrl": "/content/tutorials/2_image_classification/#table-of-contents"
  },"10": {
    "doc": "2. Image classification of museum artefacts",
    "title": "Introduction",
    "content": "In our second day, you will be asked to train an image classifier to classify images of museum artefacts. Your goal will be to recognize the artistic medium of the artefact in the image. Please download the code and data from the github repository and follow the instructions in the A1_image_classification_of_museum_artefacts. Github repository of the course . ",
    "url": "/content/tutorials/2_image_classification/#introduction",
    
    "relUrl": "/content/tutorials/2_image_classification/#introduction"
  },"11": {
    "doc": "2. Image classification of museum artefacts",
    "title": "Dataset",
    "content": "To train your machine learning image classifier, you will use the Museum Art Medium (MAMe) dataset, that contains high and low resolution images of artefacts from: . | The Metropolitan Museum of Art of New York | The Los Angeles County Museum of Art | The Cleveland Museum of Art | . The dataset is available at MAMe dataset and includes: . | Artefact images: ~37,000 in both high and low resolution (224x224 pixels) | Metadata (MAMe_dataset.csv): the museum provenance, the widht, height, product size, and aspect ratio. | The artistic medium (MAMe_labels.csv): 29 categories of artistic medium (i.e. materials and techniques), such as “oil on canvas”, “painting”, “photograph”, “ceramic”, etc. | . In addition, we provide the file data/MAMe_dataset_extended.csv, which is similar to MAMe_dataset.csv but we added an extra column that contains the MobileNetV1 features for each image. MobileNetV1 is a generalist and pre-trained neural network specialized to extract visual features. It has been trained on 1000 categories of images from the ImageNet dataset. **Assignment**: Your goal is to train the most accurate model to predict the medium of an museum artefact. Beside vision-related data, you are allowed to use the dimension of the object (width, height etc.) in your features. You must document all steps of the development cycle: data collection, data processing, feature selection, training, and evaluation. | Figures are encouraged and should be commented and interpreted | Every choice must be discussed and explained | . ",
    "url": "/content/tutorials/2_image_classification/#dataset",
    
    "relUrl": "/content/tutorials/2_image_classification/#dataset"
  },"12": {
    "doc": "2. Image classification of museum artefacts",
    "title": "2. Image classification of museum artefacts",
    "content": " ",
    "url": "/content/tutorials/2_image_classification/",
    
    "relUrl": "/content/tutorials/2_image_classification/"
  },"13": {
    "doc": "3. Mapping gestures to sounds from demonstrations",
    "title": "Day 3 - Mapping gestures to sounds from demonstrations",
    "content": "Photography from Corpus Nil from the artist Marco Donnarumma (IT/DE). The artist is using muscle contraction sensors to generate sounds. Link to the video: Corpus Nil . ",
    "url": "/content/tutorials/3_gesture_to_sound/#day-3---mapping-gestures-to-sounds-from-demonstrations",
    
    "relUrl": "/content/tutorials/3_gesture_to_sound/#day-3---mapping-gestures-to-sounds-from-demonstrations"
  },"14": {
    "doc": "3. Mapping gestures to sounds from demonstrations",
    "title": "Table of contents",
    "content": ". | Introduction | Content of the repository | Assigment | Installation required . | Plugdata | ml-lib | Streaming data from your phone | . | . ",
    "url": "/content/tutorials/3_gesture_to_sound/#table-of-contents",
    
    "relUrl": "/content/tutorials/3_gesture_to_sound/#table-of-contents"
  },"15": {
    "doc": "3. Mapping gestures to sounds from demonstrations",
    "title": "Introduction",
    "content": "The third day of this class will show you: . | A novel application of machine learning in culture and art: mapping gestures to sounds from demonstrations. | A novel graphical programming environment to develop interactive prototypes: PureData | . Your goal will be to train a machine learning model to generate sounds from gestures. The training data will be collected by you, on spot, using the sensors from your smartphone. Please download the code and data from the github repository and follow the instructions in the A3_gesture_to_sound_mapping. Github repository of the course . ",
    "url": "/content/tutorials/3_gesture_to_sound/#introduction",
    
    "relUrl": "/content/tutorials/3_gesture_to_sound/#introduction"
  },"16": {
    "doc": "3. Mapping gestures to sounds from demonstrations",
    "title": "Content of the repository",
    "content": "The repository contains the following folders: . | puredata_cheatsheet.pd: A patch with the main objects seen during the course and their keyboard shortcuts. | dub_siren_box: A practical and fun exercise to start apply the concepts seen before. The patch reimplement this hardware. | mapping_by_demonstration: Demonstration of how to map gestures to sound using the ml-lib library. | . ",
    "url": "/content/tutorials/3_gesture_to_sound/#content-of-the-repository",
    
    "relUrl": "/content/tutorials/3_gesture_to_sound/#content-of-the-repository"
  },"17": {
    "doc": "3. Mapping gestures to sounds from demonstrations",
    "title": "Assigment",
    "content": "Implement a musical instrument using Plugdata and ml-lib. The instrument can map any signal to any sound or sound effect you’d like. You are then assigned to upload the files along with a short video of your instrument in action ! . ",
    "url": "/content/tutorials/3_gesture_to_sound/#assigment",
    
    "relUrl": "/content/tutorials/3_gesture_to_sound/#assigment"
  },"18": {
    "doc": "3. Mapping gestures to sounds from demonstrations",
    "title": "Installation required",
    "content": "Plugdata . Plugdata is a free/open-source visual programming environment based on pure-data. It is available for a wide range of operating systems. Please install plugdata for your operating system from the plugdata website. ml-lib . ml-lib is a library of machine learning externals for Max and Pure Data designed and developed by Ali Momeni and Jamie Bullock. The goal of ml-lib is to provide a simple, consistent interface to a wide range of machine learning techniques in Max and Pure Data. The canonical NIME 2015 paper on ml-lib can be found here. Full class documentation can be found here. Please download the latest release of ml-lib for your operating system from the ml-lib website. Then copy the files into the PureData folder. | On Windows, you can copy the files into the folder C:\\Program Files\\plugdata\\Extra\\ml.lib | On MacOS, you can copy the files into the folder Documents/plugdata/Extra/ml.lib | . Streaming data from your phone . To stream sensors’ data from your phone via the protocol OSC to your computer, you can use the following open-source applications: . | On Android: Sensors2OSC can be installed via F-Droid | On iPhone: Data OSC can be installed via the App Store | . Both applications are free, open-source, and developed by independent developers. For this reason, it is possible that they trigger security alerts on your phone. ",
    "url": "/content/tutorials/3_gesture_to_sound/#installation-required",
    
    "relUrl": "/content/tutorials/3_gesture_to_sound/#installation-required"
  },"19": {
    "doc": "3. Mapping gestures to sounds from demonstrations",
    "title": "3. Mapping gestures to sounds from demonstrations",
    "content": " ",
    "url": "/content/tutorials/3_gesture_to_sound/",
    
    "relUrl": "/content/tutorials/3_gesture_to_sound/"
  },"20": {
    "doc": "4. Melody generation using machine learning",
    "title": "Day 4 - Melody generation using machine learning",
    "content": ". ",
    "url": "/content/tutorials/4_melody_generation/#day-4---melody-generation-using-machine-learning",
    
    "relUrl": "/content/tutorials/4_melody_generation/#day-4---melody-generation-using-machine-learning"
  },"21": {
    "doc": "4. Melody generation using machine learning",
    "title": "Table of contents",
    "content": ". Please refers to the A4_melody_generation folder in the github repository and follow the instructions in the first notebook. ",
    "url": "/content/tutorials/4_melody_generation/#table-of-contents",
    
    "relUrl": "/content/tutorials/4_melody_generation/#table-of-contents"
  },"22": {
    "doc": "4. Melody generation using machine learning",
    "title": "4. Melody generation using machine learning",
    "content": " ",
    "url": "/content/tutorials/4_melody_generation/",
    
    "relUrl": "/content/tutorials/4_melody_generation/"
  },"23": {
    "doc": "5. Visualization of semantic similarities of painters' biographies",
    "title": "Day 5 - Mapping gestures to sounds from demonstrations",
    "content": ". ",
    "url": "/content/tutorials/5_semantic_similarities_visualization/#day-5---mapping-gestures-to-sounds-from-demonstrations",
    
    "relUrl": "/content/tutorials/5_semantic_similarities_visualization/#day-5---mapping-gestures-to-sounds-from-demonstrations"
  },"24": {
    "doc": "5. Visualization of semantic similarities of painters' biographies",
    "title": "Table of contents",
    "content": ". Please refers to the A5_semantic_similarities_visualization folder in the github repository and follow the instructions in the first notebook. ",
    "url": "/content/tutorials/5_semantic_similarities_visualization/#table-of-contents",
    
    "relUrl": "/content/tutorials/5_semantic_similarities_visualization/#table-of-contents"
  },"25": {
    "doc": "5. Visualization of semantic similarities of painters' biographies",
    "title": "5. Visualization of semantic similarities of painters' biographies",
    "content": " ",
    "url": "/content/tutorials/5_semantic_similarities_visualization/",
    
    "relUrl": "/content/tutorials/5_semantic_similarities_visualization/"
  },"26": {
    "doc": "6. Analysis and creative visualization of text conversations",
    "title": "Day 6 - Analysis and creative visualization of text conversations",
    "content": ". ",
    "url": "/content/tutorials/6_conversation_analysis_and_visualization/#day-6---analysis-and-creative-visualization-of-text-conversations",
    
    "relUrl": "/content/tutorials/6_conversation_analysis_and_visualization/#day-6---analysis-and-creative-visualization-of-text-conversations"
  },"27": {
    "doc": "6. Analysis and creative visualization of text conversations",
    "title": "Table of contents",
    "content": ". Please refers to the A6_conversation_analysis_and_visualization folder in the github repository and follow the instructions in the first notebook. ",
    "url": "/content/tutorials/6_conversation_analysis_and_visualization/#table-of-contents",
    
    "relUrl": "/content/tutorials/6_conversation_analysis_and_visualization/#table-of-contents"
  },"28": {
    "doc": "6. Analysis and creative visualization of text conversations",
    "title": "6. Analysis and creative visualization of text conversations",
    "content": " ",
    "url": "/content/tutorials/6_conversation_analysis_and_visualization/",
    
    "relUrl": "/content/tutorials/6_conversation_analysis_and_visualization/"
  },"29": {
    "doc": "Tools and credits",
    "title": "Generalist tool for ML",
    "content": "Marcelle . Marcelle is a modular open source toolkit for programming interactive machine learning applications. Marcelle is built around components embedding computation and interaction that can be composed to form reactive machine learning pipelines and custom user interfaces. This architecture enables rapid prototyping and extension. Marcelle can be used to build interfaces to Python scripts, and it provides flexible data stores to facilitate collaboration between machine learning experts, designers and end users. Jules Françoise, Baptiste Caramiaux, Téo Sanchez. Marcelle: Composing Interactive Machine Learning Workflows and Interfaces. Annual ACM Symposium on User Interface Software and Technology (UIST ’21), Oct 2021, Virtual. DOI: 10.1145/3472749.3474734. PDF . Wekinator . The Wekinator is free, open source software that allows anyone to use machine learning to build new musical instruments, gestural game controllers, computer vision or computer listening systems, and more. The Wekinator allows users to build new interactive systems by demonstrating human actions and computer responses, instead of writing programming code. Fiebrink, R., &amp; Cook, P. R. (2010, January). The Wekinator: a system for real-time, interactive machine learning in music. In Proceedings of The Eleventh International Society for Music Information Retrieval Conference (ISMIR 2010)(Utrecht) (Vol. 3, pp. 2-1). ml-lib . ml-lib is a library of machine learning externals for Max and Pure Data. ml-lib is primarily based on the Gesture Recognition Toolkit by Nick Gillian ml-lib is designed to work on a variety of platforms including OS X, Windows, Linux, on Intel and ARM architectures. The goal of ml-lib is to provide a simple, consistent interface to a wide range of machine learning techniques in Max and Pure Data. Bullock, J., &amp; Momeni, A. (2015, May). Ml. lib: robust, cross-platform, open-source machine learning for max and pure data. In NIME (pp. 265-270). nn~ . nn~ is a Pd or Max/MSP external object that allows to load and run neural networks in real-time. It is based on the PyTorch C++ API and can load any network that can be exported from PyTorch to TorchScript. It can be used to load RAVE models. ",
    "url": "/content/credits/#generalist-tool-for-ml",
    
    "relUrl": "/content/credits/#generalist-tool-for-ml"
  },"30": {
    "doc": "Tools and credits",
    "title": "Specialized ML tools (audio, text, others…)",
    "content": "RAVE . Rave is a variational autoencoder for fast and high-quality neural audio synthesis developed by Antoine Caillon and Philippe Esling from IRCAM. Caillon, A., &amp; Esling, P. (2021). RAVE: A variational autoencoder for fast and high-quality neural audio synthesis. arXiv preprint arXiv:2111.05011. LangChain . LangChain is a framework for developing applications powered by language models. It enables applications that: . | Are context-aware: connect a language model to sources of context (prompt instructions, few shot examples, content to ground its response in, etc.) . | Reason: rely on a language model to reason (about how to answer based on provided context, what actions to take, etc.) . | . Flucoma . Combination of digital signal processing and machine learning. Connection to SuperCollider, PureData and Max. ",
    "url": "/content/credits/#specialized-ml-tools-audio-text-others",
    
    "relUrl": "/content/credits/#specialized-ml-tools-audio-text-others"
  },"31": {
    "doc": "Tools and credits",
    "title": "Generalist tools for audio and visual programming",
    "content": "Pure Data . Pure Data (or just “Pd”) is an open source visual programming language for multimedia. Pure Data allows you to create and manipulate audio systems using visual elements, rather than writing code. Think of it as building with virtual blocks – simply connect them together to design your unique audio setups. Plug Data . plugdata is a free/open-source visual programming environment based on pure-data. It is available for a wide range of operating systems, and can be used both as a standalone app, or as a VST3, LV2, CLAP or AU plugin. We recommend using Plug Data rather than Pure Data, as it provides a more user-friendly interface. SuperCollider . SuperCollider is a platform for audio synthesis and algorithmic composition, used by musicians, artists, and researchers working with sound. It is free and open source software available for Windows, macOS, and Linux. P5hs Processing Nannou . Visual Creative Coding, Coupling between Audio and Visual . TouchDesigner . Node-based visual programming language and environment for real-time interaction with different media . ",
    "url": "/content/credits/#generalist-tools-for-audio-and-visual-programming",
    
    "relUrl": "/content/credits/#generalist-tools-for-audio-and-visual-programming"
  },"32": {
    "doc": "Tools and credits",
    "title": "More AIArtists tools and ressources",
    "content": "AIartists . ",
    "url": "/content/credits/#more-aiartists-tools-and-ressources",
    
    "relUrl": "/content/credits/#more-aiartists-tools-and-ressources"
  },"33": {
    "doc": "Tools and credits",
    "title": "Tools and credits",
    "content": " ",
    "url": "/content/credits/",
    
    "relUrl": "/content/credits/"
  },"34": {
    "doc": "Directions and useful infos",
    "title": "How to go to the Wavelab ?",
    "content": "Address . Wavelab, Barerstraße 19, 80333 München . Copy Address . ",
    "url": "/content/directions/#how-to-go-to-the-wavelab-",
    
    "relUrl": "/content/directions/#how-to-go-to-the-wavelab-"
  },"35": {
    "doc": "Directions and useful infos",
    "title": "Arrival methods",
    "content": "Bicycle . | Bicycle parking is available. | . Public Transportation . | Subway: Königsplatz or Odeonsplatz stations. | Tram: Karolinenplatz stop. | City-Railway: Stachus or Hauptbahnhof stations (then a 10-minute walk). | . ",
    "url": "/content/directions/#arrival-methods",
    
    "relUrl": "/content/directions/#arrival-methods"
  },"36": {
    "doc": "Directions and useful infos",
    "title": "Route to Wavelab from Barerstraße 19",
    "content": ". | Enter through the left side entrance of the building and proceed to the green courtyard. Look for and follow the red HMTM signs. | Walk along the building until you see a wheelchair ramp and stairs on your right. This leads into the Wavelab. | The door to Wavelab is not automatic. Please ring the bell labeled Wavelab to gain entry. | . ",
    "url": "/content/directions/#route-to-wavelab-from-barerstra%C3%9Fe-19",
    
    "relUrl": "/content/directions/#route-to-wavelab-from-barerstraße-19"
  },"37": {
    "doc": "Directions and useful infos",
    "title": "Important Advice",
    "content": "The Wavelab is situated in the rear building of the Israeli Consulate General (GKI), which has a high security level. When visiting, please keep the following in mind: . | Avoid lingering or standing on the sidewalk at Barerstraße 19; move close to the tram stop if you need to make a phone call or are waiting for someone. | If approached by GKI security personnel, promptly identify yourself as a guest of the Wavelab. | . ",
    "url": "/content/directions/#important-advice",
    
    "relUrl": "/content/directions/#important-advice"
  },"38": {
    "doc": "Directions and useful infos",
    "title": "Directions and useful infos",
    "content": " ",
    "url": "/content/directions/",
    
    "relUrl": "/content/directions/"
  },"39": {
    "doc": "About",
    "title": "AI in culture and arts -  Human-AI interaction (tech crash course)",
    "content": " ",
    "url": "/#ai-in-culture-and-arts----human-ai-interaction-tech-crash-course",
    
    "relUrl": "/#ai-in-culture-and-arts----human-ai-interaction-tech-crash-course"
  },"40": {
    "doc": "About",
    "title": "📰 Announcements",
    "content": "15.01.2023 - Save the date! The tech crash course on AI in Culture and Arts is coming soon. First bloc will be on Tuesday, April 23rd and Thursday, April 25th, 2024. ",
    "url": "/#-announcements",
    
    "relUrl": "/#-announcements"
  },"41": {
    "doc": "About",
    "title": "Table of contents",
    "content": ". | What is AICA? | What is the crash course on Human-AI interaction ? | Learning outcomes | Prerequisites | Course content | Tutorials and teaching methods | Evaluation and ECTS | License | . ",
    "url": "/#table-of-contents",
    
    "relUrl": "/#table-of-contents"
  },"42": {
    "doc": "About",
    "title": "What is AICA?",
    "content": "The Digitization College “Artificial Intelligence in Culture and Arts” (AICA) aims to equip students at the University of Music and Performing Arts Munich (HMTM) and Hochschule München University of Applied Sciences (HM) with necessary skills to impact AI innovations in the creative and cultural industries. Learn more about AICA . ",
    "url": "/#what-is-aica",
    
    "relUrl": "/#what-is-aica"
  },"43": {
    "doc": "About",
    "title": "What is the crash course on Human-AI interaction ?",
    "content": "Artificial intelligence (AI) is increasingly impacting the cultural and creative sectors. In particular, machine learning algorithms can now generate unprecedented synthetic media, transforming how we create, produce, and distribute art and culture. Students must develop a theoretical and practical understanding of machine learning to comprehend such transformative technology and foster the development of meaningful human-AI interactions. This course addresses this need and delves into interactive machine learning for the cultural and creative sectors. The course is intended for art, cultural management, design, and computer science students. After this course, students will master the theoretical and technological foundations of machine learning, be able to train and (critically) evaluate machine learning models, and deploy them in meaningful interactive systems. The course is structured in three 2-day blocks (6 days in total). Each block provides theoretical lectures and hands-on activities to develop interactive machine-learning systems for image, sound, and text-based applications in the creative and cultural sectors. Every teaching day starts with a lecture and discussion in the morning, followed by a hands-on session on the same topic in the afternoon. The AICA tech crash course will be hosted at the Wavelab, in the Summer Semester, starting April 2024. ",
    "url": "/#what-is-the-crash-course-on-human-ai-interaction-",
    
    "relUrl": "/#what-is-the-crash-course-on-human-ai-interaction-"
  },"44": {
    "doc": "About",
    "title": "Learning outcomes",
    "content": "After successful participation in this course, students are able to: . | Understand the history and current state of AI: students will be able to explain the different waves of AI (symbolic, connectionist), precisely identify machine learning algorithms, and ex- plain their distinctive characteristics (dataset, optimization, loss, etc.). | Trainand(critically)evaluateamachinelearningalgorithm:studentswillbeabletoexplain and apply the main steps of the development cycle of machine learning, from data collection, analysis, preprocessing, training, and evaluation. They will be able to critically examine a lear- ning curve and performance metrics to assess the performance of their machine-learning mo- dels. Furthermore, they will be able to critically discuss the limitations of their model from the content of their dataset and from the perspective of bias and fairness. | Create interactive machine learning systems: students can design and implement interac- tive machine learning systems for image, sound, and text-based applications in the creative and cultural sectors. Three examples of interactive systems will be showcased in this course: a teachable image classifier, a gesture-to-sound synthesizer, and a tool for semantic and multi- modal exploration of museum archives. Students already familiar with programming and ma- chine learning will be able to dive deeper into the design and development of novel interactions with machine learning algorithms. | . ",
    "url": "/#learning-outcomes",
    
    "relUrl": "/#learning-outcomes"
  },"45": {
    "doc": "About",
    "title": "Prerequisites",
    "content": "The module is designed as an interdisciplinary venue that brings together a range of perspective. It is aimed at all students enrolled in a third-year Bachelor’s program at Hochschule München University of Applied Sciences (HM) or the Hochschule für Musik und Theater München (HMTM). Students in Master’s programs are also welcome. Students with prior computer science and machine learning knowledge will be assigned dedicated and more advanced activities to develop interactive ML systems using the open source Marcelle toolkit. To apply, please refer to the Subscription section. ",
    "url": "/#prerequisites",
    
    "relUrl": "/#prerequisites"
  },"46": {
    "doc": "About",
    "title": "Course content",
    "content": "Structured over three 2-day blocks (6 days in total), the course addresses: . | Image: This introductory block focuses on image classification using machine learning. After a general introduction to AI’s history and current state, participants will explore the machine learning development cycle, engaging with dedicated interactive applications (made with Marcelle) and computational notebooks in Python. The hands-on session will focus on training and evaluating museum artifacts using open-access and open-source datasets (MAMe, Smithsonian Open Acces). | Sound: The second block centers on musical applications. Students will be guided to create a regression model from physical gestures to sound using an open-source visual programming language for music and art (Pure Data). Participants will learn Pure Data basics and discover how to transform their smartphones into synthesizers. Students will also have the opportunity to tackle symbolic music generation using traditional programming (computational notebooks in Python). | Text: Building on the first block, this third bloc explores the use of machine learning to “embed” and navigate cultural archives. Students will use multi-modal models that link images to textual descriptions to design interactive tools for exploring and retrieving artifacts in museum archives. The more advanced students will be able to train their own embedding models on personalized datasets in Python. | . ",
    "url": "/#course-content",
    
    "relUrl": "/#course-content"
  },"47": {
    "doc": "About",
    "title": "Tutorials and teaching methods",
    "content": "This website provides a range of tutorials and ressources, organized by topic and in increasing order of difficulty. Tutorial overview . A typical day of teaching starts with a lecture on a topic, followed by a hands-on session where students can apply the concepts learned in the lecture. The hands-on sessions are based on the tutorials provided on this website. ",
    "url": "/#tutorials-and-teaching-methods",
    
    "relUrl": "/#tutorials-and-teaching-methods"
  },"48": {
    "doc": "About",
    "title": "Evaluation and ECTS",
    "content": "You will earn 2 ECTS for the validation of the course. The evaluation will be based on: . | Attendance: you must attend at least 4/6 days of teaching. Attending the first day of the course is mandatory. | Completion of in-class practical work: . | HM students must submit 3/5 of the completed practical work by the end of the course. The first assignment is mandatory. | HMTM students must submit the first assignment A1. | . | . If you do not finish during the in-class sessions, you will have to finish it at home. Asigments must be uploaded all at once on the following link and before the 10th of July. ",
    "url": "/#evaluation-and-ects",
    
    "relUrl": "/#evaluation-and-ects"
  },"49": {
    "doc": "About",
    "title": "License",
    "content": "The new teaching material (tutorials and code) created for the course is available under the Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0). Each tool and library demonstrated in the tutorials is subject to its own license. ",
    "url": "/#license",
    
    "relUrl": "/#license"
  },"50": {
    "doc": "About",
    "title": "About",
    "content": " ",
    "url": "/",
    
    "relUrl": "/"
  },"51": {
    "doc": "Provisional schedule",
    "title": "Provisional schedule",
    "content": ". | 10:00 AM | 10:30 AM | 11:00 AM | 11:30 AM | 12:00 PM | 12:30 PM | 1:00 PM | 1:30 PM | 2:00 PM | 2:30 PM | 3:00 PM | 3:30 PM | 4:00 PM | 4:30 PM | 5:00 PM | . | ",
    "url": "/content/program/",
    
    "relUrl": "/content/program/"
  },"52": {
    "doc": "Provisional schedule",
    "title": "Block 1",
    "content": "| ",
    "url": "/content/program/",
    
    "relUrl": "/content/program/"
  },"53": {
    "doc": "Provisional schedule",
    "title": "23th of April",
    "content": ". | Brief : Module introduction 10:00 AM–10:30 AM | Lecture : Introduction to machine learning 10:30 AM–12:00 PM | Hands-on : Train your first classifier with Marcelle! 1:00 PM–2:00 PM | Hands-on : Data science and machine learning tutorials 2:00 PM–5:00 PM | . | ",
    "url": "/content/program/",
    
    "relUrl": "/content/program/"
  },"54": {
    "doc": "Provisional schedule",
    "title": "25th of April",
    "content": ". | Lecture : Recap on the development cycle of ML and presentation of the first assignment 10:00 AM–10:30 AM | Assignment : Train an image classifier on museum archives in Python 10:30 AM–12:00 PM | Assignment : Train an image classifier on museum archives in Python 1:00 PM–4:30 PM | Brief : Feedback session 4:30 PM–5:00 PM | . | ",
    "url": "/content/program/",
    
    "relUrl": "/content/program/"
  },"55": {
    "doc": "Provisional schedule",
    "title": "Block 2",
    "content": "| ",
    "url": "/content/program/",
    
    "relUrl": "/content/program/"
  },"56": {
    "doc": "Provisional schedule",
    "title": "4th of June",
    "content": ". | Hands-on : Introduction to Pure Data 10:00 AM–12:00 PM | Assignment : Build a gesture-to-sound synthesizer in Pure Data 1:00 PM–5:00 PM | . | ",
    "url": "/content/program/",
    
    "relUrl": "/content/program/"
  },"57": {
    "doc": "Provisional schedule",
    "title": "6th of June",
    "content": ". | Lecture : Symbolic music generation with ML (Dr. Benedikt Zönnchen) 10:00 AM–11:00 AM | Assignment : Generative MIDI with ML in Python 11:00 AM–12:00 PM | Assignment : Generative MIDI with ML in Python 1:00 PM–5:00 PM | . | ",
    "url": "/content/program/",
    
    "relUrl": "/content/program/"
  },"58": {
    "doc": "Provisional schedule",
    "title": "Block 3",
    "content": "| ",
    "url": "/content/program/",
    
    "relUrl": "/content/program/"
  },"59": {
    "doc": "Provisional schedule",
    "title": "11th of June",
    "content": ". | Lecture : AI in the art scene (Lecture by Helena Held) 10:00 AM–11:00 AM | Assignment : Analyse and explore AI artists biographies with ML 11:00 AM–12:00 PM | Assignment : Analyse and explore AI artists biographies with ML 1:00 PM–5:00 PM | . | ",
    "url": "/content/program/",
    
    "relUrl": "/content/program/"
  },"60": {
    "doc": "Provisional schedule",
    "title": "13th of June",
    "content": ". | Assignment : Analyse and explore museum archives with ML 10:00 AM–12:00 PM | Assignment : Analyse and explore museum archives with ML 1:00 PM–5:00 PM | . | . ",
    "url": "/content/program/",
    
    "relUrl": "/content/program/"
  },"61": {
    "doc": "Instructors",
    "title": "Scientific instructors",
    "content": "Scientific instructors prepared the technical content hosted on this website. They can guide you through the tutorials and your final project implementation. Dr. Benedikt Zönnchen (HM - MUC.DAI) . Dr. Téo Sanchez (HM - MUC.DAI) . ",
    "url": "/content/staff/#scientific-instructors",
    
    "relUrl": "/content/staff/#scientific-instructors"
  },"62": {
    "doc": "Instructors",
    "title": "Art and culture instructors",
    "content": "Artistic instructors can guide you through the artistic and cultural aspects of your project. Helena Held (HMTM) . ",
    "url": "/content/staff/#art-and-culture-instructors",
    
    "relUrl": "/content/staff/#art-and-culture-instructors"
  },"63": {
    "doc": "Instructors",
    "title": "AICA project leader",
    "content": "Dr. Esther Fee Feichtner (HMTM) . ",
    "url": "/content/staff/#aica-project-leader",
    
    "relUrl": "/content/staff/#aica-project-leader"
  },"64": {
    "doc": "Instructors",
    "title": "Teaching assistants",
    "content": "Students from HM and HMTM will also be present on-site to help organizing the course. David Kosian (HMTM) . David Helm (HM) . ",
    "url": "/content/staff/#teaching-assistants",
    
    "relUrl": "/content/staff/#teaching-assistants"
  },"65": {
    "doc": "Instructors",
    "title": "Instructors",
    "content": " ",
    "url": "/content/staff/",
    
    "relUrl": "/content/staff/"
  },"66": {
    "doc": "Subscription",
    "title": "Enroll now! 💥",
    "content": "Please follow these steps to enroll in the class. ",
    "url": "/content/subscription/#enroll-now-",
    
    "relUrl": "/content/subscription/#enroll-now-"
  },"67": {
    "doc": "Subscription",
    "title": "1. Express your interest",
    "content": "Express your interest to Dr. Téo Sanchez directly (teo [dot] sanchez [at] hm [dot] edu) with a short motivation statement. ",
    "url": "/content/subscription/#1-express-your-interest",
    
    "relUrl": "/content/subscription/#1-express-your-interest"
  },"68": {
    "doc": "Subscription",
    "title": "2. Sign up on the administrative platform of your university",
    "content": "To validate the class and get the credits, you need to sign up to the class on the administrative platform of your university. Hochschule München University of applied sciences . Sign up on Nine Hochschule für Musik und Theater München . Sign up on eCampus ",
    "url": "/content/subscription/#2-sign-up-on-the-administrative-platform-of-your-university",
    
    "relUrl": "/content/subscription/#2-sign-up-on-the-administrative-platform-of-your-university"
  },"69": {
    "doc": "Subscription",
    "title": "3. Sign up to the moodle platform",
    "content": "The moodle platform is open to both HM and HMTM students. It is not an administrative platform, but tool to exchange between students and teachers. It is meant to gather all institutional emails, submit and collect assigments, and publish grades. Sign up on Moodle NB: You can subscribe with and without HM credentials. Just select DFN-AAI/eduGAIN if you are not an HM student. ",
    "url": "/content/subscription/#3-sign-up-to-the-moodle-platform",
    
    "relUrl": "/content/subscription/#3-sign-up-to-the-moodle-platform"
  },"70": {
    "doc": "Subscription",
    "title": "Subscription",
    "content": " ",
    "url": "/content/subscription/",
    
    "relUrl": "/content/subscription/"
  },"71": {
    "doc": "Tutorials",
    "title": "Tutorials",
    "content": "Important note . This section contains the tutorials of the class and will be updated before and during the course . ",
    "url": "/docs/tutorials",
    
    "relUrl": "/docs/tutorials"
  }
}
